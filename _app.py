import os
os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'
import numpy as np
import pandas as pd

from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
import faiss
import time as ttime

import streamlit as st

# ê²½ë¡œ ì„¤ì •
data_path = './data'
module_path = './modules'

# Gemini ì„¤ì •
import google.generativeai as genai

# import shutil
# os.makedirs("/root/.streamlit", exist_ok=True)
# shutil.copy("secrets.toml", "/root/.streamlit/secrets.toml")

# GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]

GOOGLE_API_KEY = ""
genai.configure(api_key=GOOGLE_API_KEY)

# í•„í„°ë§ ê¸°ì¤€ - ë‚˜ì¤‘ì—” ì¢€ ì¡°ì ˆí•˜ê¸´í•´ì•¼í•¨. ë‹¤ í‘¸ëŠ”ê±´ ë³„ë¡œê² ì§€?
safe = [
    {
        "category": "HARM_CATEGORY_HARASSMENT",
        "threshold": "BLOCK_NONE",
    },
    {
        "category": "HARM_CATEGORY_HATE_SPEECH",
        "threshold": "BLOCK_NONE",
    },
    {
        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
        "threshold": "BLOCK_NONE",
    },
    {
        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
        "threshold": "BLOCK_NONE",
    },
]

# Gemini ëª¨ë¸ ì„ íƒ
model = genai.GenerativeModel("gemini-1.5-flash", safe, )



# CSV íŒŒì¼ ë¡œë“œ
## ìì²´ ì „ì²˜ë¦¬ë¥¼ ê±°ì¹œ ë°ì´í„° íŒŒì¼ í™œìš©
csv_file_path = "JEJU_MCT_DATA_modified.csv"
df = pd.read_csv(os.path.join(data_path, csv_file_path))

# ìµœì‹ ì—°ì›” ë°ì´í„°ë§Œ ê°€ì ¸ì˜´
df = df[df['ê¸°ì¤€ì—°ì›”'] == df['ê¸°ì¤€ì—°ì›”'].max()].reset_index(drop=True)


# Streamlit App UI

st.set_page_config(page_title="ğŸŠì°¸ì‹ í•œ ì œì£¼ ë§›ì§‘!")

# Replicate Credentials
with st.sidebar:
    st.title("ğŸŠì°¸ì‹ í•œ! ì œì£¼ ë§›ì§‘")

    st.write("")

    st.subheader("ì–¸ë“œë ˆ ê°€ì‹ ë””ê°€?")

    # selectbox ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stSelectbox label {  /* This targets the label element for selectbox */
            display: none;  /* Hides the label element */
        }
        .stSelectbox div[role='combobox'] {
            margin-top: -20px; /* Adjusts the margin if needed */
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    time = st.sidebar.selectbox("", ["ì•„ì¹¨", "ì ì‹¬", "ì˜¤í›„", "ì €ë…", "ë°¤"], key="time")

    st.write("")

    st.subheader("ì–´ë“œë ˆê°€ ë§˜ì— ë“œì‹ ë””ê°€?")

    # radio ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stRadio > label {
            display: none;
        }
        .stRadio > div {
            margin-top: -20px;
        }
        </style>
        """,
        unsafe_allow_html=True
    )

    local_choice = st.radio(
        '',
        ('ì œì£¼ë„ë¯¼ ë§›ì§‘', 'ê´€ê´‘ê° ë§›ì§‘')
    )

    st.write("")

st.title("í˜¼ì € ì˜µì„œì˜ˆ!ğŸ‘‹")
st.subheader("êµ°ë§›ë‚œ ì œì£¼ ë°¥ì§‘ğŸ§‘â€ğŸ³ ì¶”ì²œí•´ë“œë¦´ê²Œì˜ˆ")

st.write("")

st.write("#í‘ë¼ì§€ #ê°ˆì¹˜ì¡°ë¦¼ #ì˜¥ë”êµ¬ì´ #ê³ ì‚¬ë¦¬í•´ì¥êµ­ #ì „ë³µëšë°°ê¸° #í•œì¹˜ë¬¼íšŒ #ë¹™ë–¡ #ì˜¤ë©”ê¸°ë–¡..ğŸ¤¤")

st.write("")

image_path = "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRTHBMuNn2EZw3PzOHnLjDg_psyp-egZXcclWbiASta57PBiKwzpW5itBNms9VFU8UwEMQ&usqp=CAU"
image_html = f"""
<div style="display: flex; justify-content: center;">
    <img src="{image_path}" alt="centered image" width="50%">
</div>
"""
st.markdown(image_html, unsafe_allow_html=True)

st.write("")

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ì–´ë“œëŸ° ì‹ë‹¹ ì°¾ìœ¼ì‹œì¿ ê³¼?"}]
st.sidebar.button('Clear Chat History', on_click=clear_chat_history)


# RAG

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"

# Hugging Faceì˜ ì‚¬ì „ í•™ìŠµëœ ì„ë² ë”© ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)

print(f'Device is {device}.')


# FAISS ì¸ë±ìŠ¤ ë¡œë“œ í•¨ìˆ˜
def load_faiss_index(index_path=os.path.join(module_path, 'faiss_index.index')):
    """a
    FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.

    Parameters:
    index_path (str): ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ.

    Returns:
    faiss.Index: ë¡œë“œëœ FAISS ì¸ë±ìŠ¤ ê°ì²´.
    """
    if os.path.exists(index_path):
        # ì¸ë±ìŠ¤ íŒŒì¼ì—ì„œ ë¡œë“œ
        index = faiss.read_index(index_path)
        print(f"FAISS ì¸ë±ìŠ¤ê°€ {index_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return index
    else:
        raise FileNotFoundError(f"{index_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# í…ìŠ¤íŠ¸ ì„ë² ë”©
def embed_text(text):
    # í† í¬ë‚˜ì´ì €ì˜ ì¶œë ¥ë„ GPUë¡œ ì´ë™
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)
    with torch.no_grad():
        # ëª¨ë¸ì˜ ì¶œë ¥ì„ GPUì—ì„œ ì—°ì‚°í•˜ê³ , í•„ìš”í•œ ë¶€ë¶„ì„ ê°€ì ¸ì˜´
        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)
    return embeddings.squeeze().cpu().numpy()  # ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™í•˜ê³  numpy ë°°ì—´ë¡œ ë³€í™˜

# ì„ë² ë”© ë¡œë“œ
embeddings = np.load(os.path.join(module_path, 'embeddings_array_file.npy'))

def generate_response_with_faiss(question, df, embeddings, model, embed_text, time, local_choice, index_path=os.path.join(module_path, 'faiss_index.index'), max_count=10, k=5, print_prompt=True):
    filtered_df = df
    
    start_time = ttime.time()
    # FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ
    index = load_faiss_index(index_path)

    # ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embed_text(question).reshape(1, -1)

    # ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (3ë°°ìˆ˜)
    distances, indices = index.search(query_embedding, k*3)

    # FAISSë¡œ ê²€ìƒ‰ëœ ìƒìœ„ kê°œì˜ ë°ì´í„°í”„ë ˆì„ ì¶”ì¶œ
    filtered_df = filtered_df.iloc[indices[0, :]].copy().reset_index(drop=True)
    end_time = ttime.time()
    latency = end_time - start_time
    print(f"Index Latency: {latency:.6f} seconds")

    # ì›¹í˜ì´ì§€ì˜ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•˜ëŠ” ì˜ì—…ì‹œê°„, í˜„ì§€ì¸ ë§›ì§‘ ì¡°ê±´ êµ¬í˜„

    # ì˜ì—…ì‹œê°„ ì˜µì…˜
    # í•„í„°ë§ ì¡°ê±´ìœ¼ë¡œ í™œìš©

    # ì˜ì—…ì‹œê°„ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ê²Œë“¤ë§Œ í•„í„°ë§
    if time == 'ì•„ì¹¨':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(5, 12)))].reset_index(drop=True)
    elif time == 'ì ì‹¬':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(12, 14)))].reset_index(drop=True)
    elif time == 'ì˜¤í›„':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(14, 18)))].reset_index(drop=True)
    elif time == 'ì €ë…':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in range(18, 23)))].reset_index(drop=True)
    elif time == 'ë°¤':
        filtered_df = filtered_df[filtered_df['ì˜ì—…ì‹œê°„'].apply(lambda x: isinstance(eval(x), list) and any(hour in eval(x) for hour in [23, 24, 1, 2, 3, 4]))].reset_index(drop=True)

    # í•„í„°ë§ í›„ ê°€ê²Œê°€ ì—†ìœ¼ë©´ ë©”ì‹œì§€ë¥¼ ë°˜í™˜
    if filtered_df.empty:
        return f"í˜„ì¬ ì„ íƒí•˜ì‹  ì‹œê°„ëŒ€({time})ì—ëŠ” ì˜ì—…í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    filtered_df = filtered_df.reset_index(drop=True).head(k)


    # í˜„ì§€ì¸ ë§›ì§‘ ì˜µì…˜

    # í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜í•˜ì—¬ í™œìš©
    if local_choice == 'ì œì£¼ë„ë¯¼ ë§›ì§‘':
        local_choice = 'ì œì£¼ë„ë¯¼(í˜„ì§€ì¸) ë§›ì§‘'
    elif local_choice == 'ê´€ê´‘ê° ë§›ì§‘':
        local_choice = 'í˜„ì§€ì¸ ë¹„ì¤‘ì´ ë‚®ì€ ê´€ê´‘ê° ë§›ì§‘'

    # ì„ íƒëœ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì²˜ë¦¬
    if filtered_df.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."


    # ì°¸ê³ í•  ì •ë³´ì™€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    reference_info = ""
    for idx, row in filtered_df.iterrows():
        reference_info += f"{row['text']}\n"

    # ì‘ë‹µì„ ë°›ì•„ì˜¤ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    # prompt = f"ì§ˆë¬¸: {question}\nì¡°ê±´: ì´ìš© ë¹„ì¤‘ì€ ì†Œìˆ˜ê°€ ì•„ë‹Œ %ë¡œ ë‚˜íƒ€ë‚´. ìœ„ì¹˜ëŠ” ì œëŒ€ë¡œ í™•ì¸í•˜ê³  ê²°ì •í•´\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µ:"
    # prompt = f"ë‚´ê°€ ì§ˆë¬¸í•  ë¬¸ì¥ì—ì„œ ì°¾ê³ ì‹¶ì€ ì§€ëª…, ë¨¹ê³ ì‹¶ì€ ìŒì‹ì¢…ë¥˜, í˜„ì¬ ìœ„ì¹˜(í˜„ì¬ìœ„ì¹˜ëŠ” ê°€ëŠ¥í•˜ë‹¤ë©´ ë¬¸ì¥ì„ í†µí•´ ì¶”ë¡ í•´)ë“±ì˜ ì •ë³´ë¥¼ ì°¾ì•„ì„œ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ ì—†ë‹¤ë©´ 0ì„ ë„£ì–´. í˜•ì‹: LOC:ì°¾ê³ ì‹¶ì€ ì§€ëª… FOOD:ìŒì‹ì¢…ë¥˜ NOW:í˜„ì¬ìœ„ì¹˜ INFO:ìš”ì²­ì‚¬í•­\nì§ˆë¬¸: {question}"
    prompt = f"ë„ˆëŠ” ë§›ì§‘ì„ ì¶”ì²œí•´ì£¼ëŠ” ì‚¬ëŒì¸ë°, ë„ˆì—ê²Œ ìŒì‹ì— ê´€í•œ ì§ˆë¬¸ì„ í• êº¼ì•¼. ë’¤ì— ì§ˆë¬¸í•  ë¬¸ì¥ì—ì„œ ê¸ì •ì ì¸ ë¶€ë¶„(ì›í•˜ëŠ” ë¶€ë¶„. ì—†ìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ” ë„£ì§€ë§ˆ) ë¶€ì •ì ì¸ ë¶€ë¶„(ì‹«ì–´í•˜ëŠ”, ì œì™¸í•  ë¶€ë¶„)ì„ ë¶„ë¦¬í•´ì„œ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜. ì°¸ê³ ë¡œ í‘ë¼ì§€ëŠ” ì œì£¼ë„ì˜ íŠ¹ì‚°í’ˆì¸ ë¼ì§€ê³ ê¸°ì¸ë° ì´ ë‚´ìš©ì„ ë’¤ì— ì¶œë ¥í•˜ì§€ ë§ì•„ì¤˜. ì§ˆë¬¸ì—ì„œ ì—†ëŠ” ë‚´ìš©ì„ ì ì§€ë§ˆ\ní˜•ì‹: POS:ê¸ì • NEG:ë¶€ì •\nì§ˆë¬¸: {question}"
    if print_prompt:
        print('-----------------------------'*3)
        print(prompt)
        print('-----------------------------'*3)



    start_time = ttime.time()
    # ì‘ë‹µ ìƒì„±
    response = model.generate_content(prompt)
    end_time = ttime.time()
    latency = end_time - start_time
    print(f"LLM Latency: {latency:.6f} seconds")
    
    return response


# User-provided prompt
if prompt := st.chat_input(): # (disabled=not replicate_api):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # response = generate_llama2_response(prompt)
            response = generate_response_with_faiss(prompt, df, embeddings, model, embed_text, time, local_choice)
            placeholder = st.empty()
            full_response = ''

            # ë§Œì•½ responseê°€ GenerateContentResponse ê°ì²´ë¼ë©´, ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
            if isinstance(response, str):
                full_response = response
            else:
                full_response = response.text  # response ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ë¶€ë¶„ ì¶”ì¶œ

            # for item in response:
            #     full_response += item
            #     placeholder.markdown(full_response)

            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)